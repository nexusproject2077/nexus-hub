"""
Version simplifi√©e qui utilise l'API Web Instagram directement
Au lieu d'Instaloader qui est probl√©matique
"""
import requests
import os
import json
import time

# Configuration
USERNAME = os.environ.get('INSTA_USERNAME', 'merickkn')
SESSION_ID = os.environ.get('INSTA_SESSION_ID', '')
OUTPUT_FILE = 'fils/followers_data.json'

def fetch_followers():
    """R√©cup√®re le nombre d'abonn√©s Instagram"""

    # Donn√©es par d√©faut
    data = {
        "timestamp": int(time.time()),
        "followers": 0,
        "status": "initial_failure"
    }

    try:
        if not SESSION_ID:
            raise ValueError("INSTA_SESSION_ID n'est pas configur√©")

        print(f"üîç R√©cup√©ration des abonn√©s pour @{USERNAME}")
        print(f"üìù Session ID: {len(SESSION_ID)} caract√®res")

        # Headers pour simuler un navigateur
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'X-IG-App-ID': '936619743392459',
            'X-Requested-With': 'XMLHttpRequest',
        }

        cookies = {
            'sessionid': SESSION_ID
        }

        # M√©thode 1: API Web Instagram (la plus fiable)
        print("\nüì° M√©thode 1: API Web Instagram...")
        url = f'https://www.instagram.com/api/v1/users/web_profile_info/?username={USERNAME}'

        response = requests.get(
            url,
            headers=headers,
            cookies=cookies,
            timeout=15
        )

        print(f"   Status: {response.status_code}")

        if response.status_code == 200:
            json_data = response.json()

            if 'data' in json_data and 'user' in json_data['data']:
                user = json_data['data']['user']
                followers = user.get('edge_followed_by', {}).get('count', 0)

                print(f"   ‚úÖ Succ√®s! Abonn√©s: {followers}")

                data['followers'] = followers
                data['status'] = 'success'
                return data

        # M√©thode 2: Scraping de la page publique (fallback)
        print("\nüì° M√©thode 2: Scraping page publique...")
        url = f'https://www.instagram.com/{USERNAME}/'

        response = requests.get(
            url,
            headers=headers,
            cookies=cookies,
            timeout=15
        )

        print(f"   Status: {response.status_code}")

        if response.status_code == 200:
            import re

            # Chercher le JSON embarqu√© dans le HTML
            match = re.search(r'"edge_followed_by":\{"count":(\d+)\}', response.text)

            if match:
                followers = int(match.group(1))
                print(f"   ‚úÖ Succ√®s! Abonn√©s: {followers}")

                data['followers'] = followers
                data['status'] = 'success_scraping'
                return data

        # Si toutes les m√©thodes √©chouent
        print("\n‚ùå Toutes les m√©thodes ont √©chou√©")

        # Garder les anciennes donn√©es si disponibles
        try:
            with open(OUTPUT_FILE, 'r') as f:
                old_data = json.load(f)
                data['followers'] = old_data.get('followers', 0)
                data['status'] = f'failed_retaining_old_data (HTTP {response.status_code})'
                print(f"   üì¶ Conservation des anciennes donn√©es: {data['followers']} abonn√©s")
        except:
            data['status'] = f'failed_all_methods (HTTP {response.status_code})'

    except requests.exceptions.Timeout:
        print("‚ùå Timeout - Instagram ne r√©pond pas")
        data['status'] = 'failed_timeout'

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur r√©seau: {e}")
        data['status'] = f'failed_network: {type(e).__name__}'

    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        data['status'] = f'failed: {type(e).__name__}'

    finally:
        # Sauvegarder les donn√©es
        try:
            os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

            with open(OUTPUT_FILE, 'w') as f:
                json.dump(data, f, indent=4)

            print(f"\nüíæ Donn√©es sauvegard√©es: {OUTPUT_FILE}")
            print(f"   Status: {data['status']}")
            print(f"   Abonn√©s: {data['followers']}")

        except Exception as write_error:
            print(f"‚ùå Erreur d'√©criture: {write_error}")

    return data

if __name__ == "__main__":
    print("=" * 60)
    print("R√âCUP√âRATION DES ABONN√âS INSTAGRAM")
    print("=" * 60)

    result = fetch_followers()

    print("\n" + "=" * 60)
    if result['status'] == 'success' or result['status'] == 'success_scraping':
        print(f"‚úÖ SUCC√àS! {result['followers']} abonn√©s")
    else:
        print(f"‚ùå √âCHEC: {result['status']}")
    print("=" * 60)
